{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPAgNiMxtdUevnEEDFaJhb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veydantkatyal/image-generator-gan/blob/main/image_generator_gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup and Imports**"
      ],
      "metadata": {
        "id": "fOwUG52RPz3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jeBplFvtW9E",
        "outputId": "5cb8d78a-92c0-4137-8c56-9faa22d796b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision matplotlib nltk tqdm gradio pycocotools --quiet\n",
        "import os, json, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Auto Download & Setup Datasets**\n",
        "we'll work with oxford 102 flowers only\n",
        "\n"
      ],
      "metadata": {
        "id": "StWeiWEsYoY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"data/oxford/images\", exist_ok=True)\n",
        "\n",
        "# Only download flower images, no captions\n",
        "!wget -q -O data/oxford/102flowers.tgz https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\n",
        "!tar -xzf data/oxford/102flowers.tgz -C data/oxford\n",
        "!mv data/oxford/jpg/* data/oxford/images/\n",
        "!rm -r data/oxford/jpg"
      ],
      "metadata": {
        "id": "6RV1DuMRYTYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizer**"
      ],
      "metadata": {
        "id": "3FKKpG9mY6O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "        self.idx2word = ['<PAD>', '<UNK>']\n",
        "\n",
        "    def build_vocab(self, captions):\n",
        "        for caption in captions:\n",
        "            for word in word_tokenize(caption.lower()):\n",
        "                if word not in self.word2idx:\n",
        "                    self.word2idx[word] = len(self.word2idx)\n",
        "                    self.idx2word.append(word)\n",
        "\n",
        "    def tokenize(self, sentence, max_len=20):\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        ids = [self.word2idx.get(w, self.word2idx['<UNK>']) for w in tokens]\n",
        "        ids += [0] * (max_len - len(ids))\n",
        "        return torch.tensor(ids[:max_len])\n",
        "\n",
        "# Build vocab\n",
        "synthetic_captions = [\n",
        "    \"a red flower with round petals\",\n",
        "    \"a yellow flower blooming in sunlight\",\n",
        "    \"a white flower with a purple center\",\n",
        "    \"a vibrant blue flower in a garden\"\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.build_vocab(synthetic_captions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JQ_HRUeYTWD",
        "outputId": "e38a1bbc-1ae3-435d-d82f-125ae38a5f34"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset with Synthetic Captions**"
      ],
      "metadata": {
        "id": "c5Bk0oNbY8TM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os, random\n",
        "\n",
        "# Dataset Class\n",
        "class OxfordFlowersDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform, tokenizer, max_len=20):\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.colors = [\"red\", \"yellow\", \"blue\", \"white\", \"purple\", \"pink\"]\n",
        "        self.shapes = [\"round\", \"oval\", \"spiky\", \"star-shaped\", \"long\", \"wide\"]\n",
        "\n",
        "    def __len__(self): return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = self.transform(Image.open(img_path).convert(\"RGB\"))\n",
        "\n",
        "        color = random.choice(self.colors)\n",
        "        shape = random.choice(self.shapes)\n",
        "        caption = f\"a {color} flower with {shape} petals\"\n",
        "        tokens = self.tokenizer.tokenize(caption, self.max_len)\n",
        "\n",
        "        return image, tokens\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Dataset & Dataloader\n",
        "dataset = OxfordFlowersDataset(\"data/oxford/images\", transform, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "shPIvW4tUS7U"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Encoder + U-Net Model**"
      ],
      "metadata": {
        "id": "dexFDZWSZI9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=256):\n",
        "        super(TextEncoder, self).__init__()  # ✅ safer & explicit\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        x = self.embedding(tokens)\n",
        "        _, h = self.rnn(x)\n",
        "        return h.squeeze(0)\n",
        "\n",
        "class CondUNet(nn.Module):\n",
        "    def __init__(self, text_dim=256):\n",
        "        super(CondUNet, self).__init__()  # ✅ explicit\n",
        "        self.text_proj = nn.Linear(text_dim, 64 * 64)\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, text_embed):\n",
        "        b = x.size(0)\n",
        "        tmap = self.text_proj(text_embed).view(b, 1, 64, 64)  # project text -> spatial\n",
        "        x = torch.cat([x, tmap], dim=1)  # concat along channel dimension\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "hDKhu-VFUS1s"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diffusion Scheduler + Training**"
      ],
      "metadata": {
        "id": "jw1l_jL5ZLZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Diffusion Schedule\n",
        "def get_noise_schedule(T=300, device='cpu'):\n",
        "    beta = torch.linspace(1e-4, 0.02, T, device=device)\n",
        "    alpha = 1 - beta\n",
        "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
        "    return beta, alpha, alpha_bar\n",
        "\n",
        "T = 300\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "beta, alpha, alpha_bar = get_noise_schedule(device=device)\n",
        "\n",
        "# Forward Diffusion Step\n",
        "def forward_diffusion(x0, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x0)\n",
        "    sqrt_alpha_bar = torch.sqrt(alpha_bar[t]).float().unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
        "    sqrt_one_minus = torch.sqrt(1 - alpha_bar[t]).float().unsqueeze(1).unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "    return sqrt_alpha_bar * x0 + sqrt_one_minus * noise, noise\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Model & Optimizer\n",
        "model = CondUNet().to(device)\n",
        "text_encoder = TextEncoder(len(tokenizer.word2idx)).to(device)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) + list(text_encoder.parameters()),\n",
        "    lr=1e-4\n",
        ")\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(100):\n",
        "    pbar = tqdm(dataloader)\n",
        "    for images, captions in pbar:\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Random time steps\n",
        "        t = torch.randint(0, T, (images.size(0),), device=device)\n",
        "\n",
        "        # Forward diffusion\n",
        "        noise = torch.randn_like(images)\n",
        "        x_noisy, noise = forward_diffusion(images, t, noise)\n",
        "\n",
        "        # Conditional prediction\n",
        "        text_embed = text_encoder(captions)\n",
        "        pred = model(x_noisy, text_embed)\n",
        "\n",
        "        # Loss + backprop\n",
        "        loss = criterion(pred, noise)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "X9JDHsxoUSzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ae1db7-5685-47c7-f14e-ecac5041772d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 | Loss: 0.1570: 100%|██████████| 256/256 [00:34<00:00,  7.46it/s]\n",
            "Epoch 2 | Loss: 0.1415: 100%|██████████| 256/256 [00:33<00:00,  7.61it/s]\n",
            "Epoch 3 | Loss: 0.1414: 100%|██████████| 256/256 [00:33<00:00,  7.60it/s]\n",
            "Epoch 4 | Loss: 0.0665: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 5 | Loss: 0.1122: 100%|██████████| 256/256 [00:33<00:00,  7.54it/s]\n",
            "Epoch 6 | Loss: 0.1272: 100%|██████████| 256/256 [00:34<00:00,  7.50it/s]\n",
            "Epoch 7 | Loss: 0.1484: 100%|██████████| 256/256 [00:33<00:00,  7.54it/s]\n",
            "Epoch 8 | Loss: 0.1355: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 9 | Loss: 0.1144: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 10 | Loss: 0.0986: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 11 | Loss: 0.0988: 100%|██████████| 256/256 [00:33<00:00,  7.62it/s]\n",
            "Epoch 12 | Loss: 0.0858: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 13 | Loss: 0.0773: 100%|██████████| 256/256 [00:34<00:00,  7.45it/s]\n",
            "Epoch 14 | Loss: 0.1035: 100%|██████████| 256/256 [00:33<00:00,  7.53it/s]\n",
            "Epoch 15 | Loss: 0.0638: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 16 | Loss: 0.0689: 100%|██████████| 256/256 [00:34<00:00,  7.52it/s]\n",
            "Epoch 17 | Loss: 0.1060: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 18 | Loss: 0.1046: 100%|██████████| 256/256 [00:33<00:00,  7.53it/s]\n",
            "Epoch 19 | Loss: 0.0871: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 20 | Loss: 0.1108: 100%|██████████| 256/256 [00:34<00:00,  7.52it/s]\n",
            "Epoch 21 | Loss: 0.0994: 100%|██████████| 256/256 [00:33<00:00,  7.61it/s]\n",
            "Epoch 22 | Loss: 0.0604: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 23 | Loss: 0.0637: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 24 | Loss: 0.1179: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 25 | Loss: 0.0888: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 26 | Loss: 0.0922: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 27 | Loss: 0.0679: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 28 | Loss: 0.0922: 100%|██████████| 256/256 [00:33<00:00,  7.60it/s]\n",
            "Epoch 29 | Loss: 0.0682: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 30 | Loss: 0.0620: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 31 | Loss: 0.1151: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 32 | Loss: 0.1397: 100%|██████████| 256/256 [00:33<00:00,  7.60it/s]\n",
            "Epoch 33 | Loss: 0.0908: 100%|██████████| 256/256 [00:33<00:00,  7.54it/s]\n",
            "Epoch 34 | Loss: 0.1114: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 35 | Loss: 0.1173: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 36 | Loss: 0.1060: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 37 | Loss: 0.0862: 100%|██████████| 256/256 [00:34<00:00,  7.49it/s]\n",
            "Epoch 38 | Loss: 0.0838: 100%|██████████| 256/256 [00:34<00:00,  7.50it/s]\n",
            "Epoch 39 | Loss: 0.1188: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 40 | Loss: 0.1008: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 41 | Loss: 0.0602: 100%|██████████| 256/256 [00:33<00:00,  7.54it/s]\n",
            "Epoch 42 | Loss: 0.0599: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 43 | Loss: 0.1356: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 44 | Loss: 0.0871: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 45 | Loss: 0.1029: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 46 | Loss: 0.0736: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 47 | Loss: 0.1257: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 48 | Loss: 0.0679: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 49 | Loss: 0.0956: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 50 | Loss: 0.1203: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 51 | Loss: 0.0674: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 52 | Loss: 0.0920: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 53 | Loss: 0.0826: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 54 | Loss: 0.0806: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 55 | Loss: 0.0657: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 56 | Loss: 0.0973: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 57 | Loss: 0.0635: 100%|██████████| 256/256 [00:33<00:00,  7.60it/s]\n",
            "Epoch 58 | Loss: 0.0435: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 59 | Loss: 0.0690: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 60 | Loss: 0.0771: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 61 | Loss: 0.1068: 100%|██████████| 256/256 [00:33<00:00,  7.61it/s]\n",
            "Epoch 62 | Loss: 0.1059: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 63 | Loss: 0.0805: 100%|██████████| 256/256 [00:33<00:00,  7.54it/s]\n",
            "Epoch 64 | Loss: 0.1171: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 65 | Loss: 0.0947: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 66 | Loss: 0.0785: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 67 | Loss: 0.0836: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 68 | Loss: 0.0668: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 69 | Loss: 0.0505: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 70 | Loss: 0.0972: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 71 | Loss: 0.0653: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 72 | Loss: 0.0773: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 73 | Loss: 0.0602: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 74 | Loss: 0.0687: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 75 | Loss: 0.0807: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 76 | Loss: 0.0451: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 77 | Loss: 0.0686: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 78 | Loss: 0.1111: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 79 | Loss: 0.0683: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 80 | Loss: 0.1070: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 81 | Loss: 0.0489: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 82 | Loss: 0.0909: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 83 | Loss: 0.0793: 100%|██████████| 256/256 [00:34<00:00,  7.52it/s]\n",
            "Epoch 84 | Loss: 0.0681: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 85 | Loss: 0.0717: 100%|██████████| 256/256 [00:34<00:00,  7.53it/s]\n",
            "Epoch 86 | Loss: 0.0734: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 87 | Loss: 0.1071: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 88 | Loss: 0.0476: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 89 | Loss: 0.0559: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 90 | Loss: 0.1117: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 91 | Loss: 0.0756: 100%|██████████| 256/256 [00:33<00:00,  7.55it/s]\n",
            "Epoch 92 | Loss: 0.1158: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 93 | Loss: 0.0844: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n",
            "Epoch 94 | Loss: 0.0971: 100%|██████████| 256/256 [00:33<00:00,  7.59it/s]\n",
            "Epoch 95 | Loss: 0.0705: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 96 | Loss: 0.0622: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 97 | Loss: 0.0846: 100%|██████████| 256/256 [00:33<00:00,  7.56it/s]\n",
            "Epoch 98 | Loss: 0.0539: 100%|██████████| 256/256 [00:33<00:00,  7.57it/s]\n",
            "Epoch 99 | Loss: 0.0833: 100%|██████████| 256/256 [00:34<00:00,  7.53it/s]\n",
            "Epoch 100 | Loss: 0.1047: 100%|██████████| 256/256 [00:33<00:00,  7.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gradio App for Prompt-Based Generation**"
      ],
      "metadata": {
        "id": "lTNH4zkXZO72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import gradio as gr\n",
        "import torchvision.transforms.functional as TF\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "# Folder to store temporary outputs\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_image_and_download(prompt):\n",
        "    model.eval()\n",
        "    text_encoder.eval()\n",
        "\n",
        "    # Tokenize and embed\n",
        "    tokens = tokenizer.tokenize(prompt, max_len=20).unsqueeze(0).to(device)\n",
        "    text_embed = text_encoder(tokens)\n",
        "\n",
        "    # Start with noise\n",
        "    x = torch.randn((1, 3, 64, 64)).to(device)\n",
        "\n",
        "    for t in reversed(range(T)):\n",
        "        t_tensor = torch.full((1,), t, device=device)\n",
        "        pred_noise = model(x, text_embed)\n",
        "        x = (1 / alpha[t].sqrt()) * (x - (beta[t] / (1 - alpha_bar[t]).sqrt()) * pred_noise)\n",
        "        if t > 0:\n",
        "            x += beta[t].sqrt() * torch.randn_like(x)\n",
        "\n",
        "    # Convert to image\n",
        "    image = x.squeeze(0).cpu().clamp(-1, 1) * 0.5 + 0.5\n",
        "    image = TF.to_pil_image(image)\n",
        "\n",
        "    # Save to file with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    file_path = f\"outputs/generated_{timestamp}.png\"\n",
        "    image.save(file_path)\n",
        "\n",
        "    return image, file_path  # Display + Download\n",
        "\n",
        "gr.Interface(\n",
        "    fn=generate_image_and_download,\n",
        "    inputs=gr.Textbox(lines=1, label=\"🌸 Describe Your Flower\", placeholder=\"e.g. a red flower with round petals\"),\n",
        "    outputs=[\n",
        "        gr.Image(type=\"pil\", label=\"🖼️ Generated Image\"),\n",
        "        gr.File(label=\"📥 Download Image\")\n",
        "    ],\n",
        "    title=\"🧠 Oxford Flowers Text-to-Image Diffusion\",\n",
        "    description=\"Describe a flower and generate a realistic image using a custom diffusion model. You can also download the result.\",\n",
        "    examples=[\n",
        "        [\"a red flower with round petals\"],\n",
        "        [\"a yellow flower in sunlight\"],\n",
        "        [\"a purple flower with star-shaped petals\"]\n",
        "    ],\n",
        "    theme=\"default\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "ZN1q9_9dUSwk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9e61e0-6059-4c06-8f3c-80bac42f952d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "torch\n",
        "torchvision\n",
        "gradio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v2b4YdFENa_",
        "outputId": "b2b3d5cc-adb9-4c19-cfbd-362f491c17b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    }
  ]
}